import os
import json
import re
import random
from pathlib import Path
from typing import List, Dict, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
from openai import OpenAI


DATA_LINE_ATTR_PATTERN = re.compile(
    r'data-line\s*=\s*(?:\\?["\'])?(?P<line>\d+)(?:\\?["\'])?',
    re.IGNORECASE,
)
P_TAG_WITH_LINE_PATTERN = re.compile(
    r'<p[^>]*data-line\s*=\s*(?:\\?["\'])?(?P<line>\d+)(?:\\?["\'])?[^>]*>.*?</p>',
    re.DOTALL | re.IGNORECASE,
)


def normalize_data_line_attribute(text: str) -> str:
    """å°‡ data-line å±¬æ€§çµ±ä¸€ç‚ºæœªè·³è„«çš„é›™å¼•è™Ÿæ ¼å¼ã€‚"""
    return DATA_LINE_ATTR_PATTERN.sub(
        lambda match: f'data-line="{match.group("line")}"',
        text,
    )


class TranslationBatchProcessor:
    def __init__(self, api_key: str, batch_size: int = 20, max_workers: int = 10):
        """åˆå§‹åŒ–ç¿»è­¯æ‰¹æ¬¡è™•ç†å™¨

        Args:
            api_key: Grok API é‡‘é‘°
            batch_size: æ¯æ‰¹è™•ç†çš„è¡Œæ•¸ (é è¨­ 20 è¡Œ)
            max_workers: ä¸¦è¡Œè™•ç†çš„æª”æ¡ˆæ•¸é‡ (é è¨­ 10)
        """
        self.client = OpenAI(api_key=api_key, base_url="https://api.x.ai/v1")
        self.batch_size = batch_size
        self.max_workers = max_workers
        self.stepc_dir = Path("stepc")
        self.stepd_dir = Path("stepd")
        self.stepe_dir = Path("stepe")
        self.stepf_dir = Path("stepf")
        self.stepg_dir = Path("stepg")
        self.stepaa_dir = Path("stepaa")
        self.lock = threading.RLock()
        self.progress_tracker = {}
        self.last_update_time = 0
        self.update_interval = 0.5

        for dir_path in [self.stepc_dir, self.stepd_dir, self.stepe_dir,
                         self.stepf_dir, self.stepg_dir, self.stepaa_dir]:
            dir_path.mkdir(exist_ok=True)

    def is_pure_chinese(self, text: str) -> bool:
        """æª¢æ¸¬æ–‡å­—æ˜¯å¦ç‚ºç´”ä¸­æ–‡ (åªåŒ…å«ä¸­æ–‡å­—ç¬¦ã€æ¨™é»å’Œç©ºæ ¼)"""
        cleaned = re.sub(r'[\s\u3000-\u303F\uFF00-\uFFEF]', '', text)
        if not cleaned:
            return False
        chinese_pattern = re.compile(r'^[\u4E00-\u9FFF]+$')
        return bool(chinese_pattern.match(cleaned))

    def has_english(self, text: str) -> bool:
        """æª¢æ¸¬æ–‡å­—ä¸­æ˜¯å¦æ®˜ç•™è‹±æ–‡"""
        text_clean = re.sub(r'https?://[^\s]+', '', text)
        text_clean = re.sub(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}', '', text_clean)
        if re.search(r'[a-zA-Z]{2,}', text_clean):
            return True
        return False

    def validate_translation_entry(self, entry: Dict) -> bool:
        """é©—è­‰ translation_dictionary æ¢ç›®æ˜¯å¦æœ‰æ•ˆ"""
        if 'en' not in entry or 'zh' not in entry:
            return False
        en_text = entry['en'].strip()
        zh_text = entry['zh'].strip()
        if not en_text or not zh_text:
            return False
        if not self.is_pure_chinese(zh_text):
            return False
        if self.has_english(zh_text):
            return False
        return True

    def clear_directory(self, directory: Path):
        """æ¸…ç©ºæŒ‡å®šç›®éŒ„ä¸‹çš„æ‰€æœ‰æª”æ¡ˆ"""
        if directory.exists():
            for file in directory.glob("*"):
                if file.is_file():
                    file.unlink()
            print(f"  ğŸ—‘ï¸ å·²æ¸…ç©º: {directory}/")

    def clear_processing_directories(self):
        """æ¸…ç©ºè™•ç†éç¨‹ä¸­çš„æš«å­˜ç›®éŒ„"""
        print(f"\n{'='*70}")
        print("ğŸ§¹ æ¸…ç©ºæš«å­˜ç›®éŒ„...")
        print(f"{'='*70}")
        self.clear_directory(self.stepe_dir)
        self.clear_directory(self.stepf_dir)
        self.clear_directory(self.stepg_dir)
        print("âœ… æš«å­˜ç›®éŒ„æ¸…ç©ºå®Œæˆ\n")

    def extract_text_from_tags(self, line: str) -> str:
        """å¾ HTML æ¨™ç±¤ä¸­æå–ç´”æ–‡å­—å…§å®¹"""
        match = re.search(r'<p[^>]*>(.*?)</p>', line, re.DOTALL)
        return match.group(1) if match else ""

    def contains_english(self, text: str) -> bool:
        """æª¢æ¸¬æ–‡å­—ä¸­æ˜¯å¦åŒ…å«è‹±æ–‡ (æ’é™¤ URLã€Email)"""
        text_without_urls = re.sub(r'https?://[^\s]+', '', text)
        text_without_urls = re.sub(r'www\.[^\s]+', '', text_without_urls)
        text_without_emails = re.sub(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}', '', text_without_urls)
        english_pattern = re.compile(r'[a-zA-Z]{3,}')
        return bool(english_pattern.search(text_without_emails))

    def needs_translation(self, line: str) -> bool:
        """åˆ¤æ–·æ˜¯å¦éœ€è¦ç¿»è­¯ (åªæª¢æŸ¥æ¨™ç±¤å…§çš„æ–‡å­—å…§å®¹)"""
        text_content = self.extract_text_from_tags(line)
        if not text_content or not text_content.strip():
            return False
        return self.contains_english(text_content)

    def extract_line_number(self, line: str) -> int:
        """å¾ HTML æ¨™ç±¤ä¸­æå–è¡Œè™Ÿ"""
        match = DATA_LINE_ATTR_PATTERN.search(line)
        return int(match.group("line")) if match else -1

    def get_translation_lines(self, lines: List[str]) -> List[Tuple[int, str, int]]:
        """ç²å–éœ€è¦ç¿»è­¯çš„è¡Œ (åŒ…å«è‹±æ–‡)"""
        translation_lines = []
        for idx, line in enumerate(lines):
            if self.needs_translation(line):
                html_line_num = self.extract_line_number(line)
                translation_lines.append((idx, line, html_line_num))
        return translation_lines

    def load_translation_dictionary(self, json_file: Path) -> List[Dict]:
        """è¼‰å…¥ç¿»è­¯å­—å…¸ (é™£åˆ—æ ¼å¼)"""
        if json_file.exists():
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    return data if isinstance(data, list) else []
            except json.JSONDecodeError:
                return []
        return []

    def save_translation_dictionary(self, json_file: Path, dictionary: List[Dict]):
        """å„²å­˜ç¿»è­¯å­—å…¸ (ç·šç¨‹å®‰å…¨ - ä½¿ç”¨ RLock å¯é‡å…¥)"""
        with self.lock:
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump(dictionary, f, ensure_ascii=False, separators=(',', ':'))

    def merge_dictionaries(self, original: List[Dict], new: List[Dict]) -> List[Dict]:
        """åˆä½µå…©å€‹å­—å…¸é™£åˆ—,åªæ·»åŠ æ–°çš„ä¸”æœ‰æ•ˆçš„æ¢ç›®"""
        existing_en = {item['en'] for item in original if 'en' in item}
        merged = original.copy()
        for item in new:
            if self.validate_translation_entry(item):
                if item['en'] not in existing_en:
                    merged.append(item)
                    existing_en.add(item['en'])
        return merged

    def create_prompt(self, lines: List[str], translation_dict: List[Dict]) -> str:
        """å»ºç«‹ Grok æç¤ºè©"""
        valid_entries = [
            entry for entry in translation_dict
            if self.validate_translation_entry(entry)
        ]
        en_lookup = {entry['en'].strip(): entry for entry in valid_entries}
        selected_entries = []
        used_en = set()
        for line in lines:
            text_content = self.extract_text_from_tags(line).strip()
            if not text_content:
                continue
            entry = en_lookup.get(text_content)
            if entry and entry['en'] not in used_en:
                selected_entries.append(entry)
                used_en.add(entry['en'])
        if len(selected_entries) < 5 and valid_entries:
            remaining = [entry for entry in valid_entries if entry['en'] not in used_en]
            random.shuffle(remaining)
            while len(selected_entries) < 5 and remaining:
                selected_entries.append(remaining.pop())
            if len(selected_entries) < 5:
                while len(selected_entries) < 5 and valid_entries:
                    selected_entries.append(random.choice(valid_entries))
        dict_json = json.dumps(selected_entries, ensure_ascii=False, separators=(',', ':'))
        content = "".join(lines)
         prompt = f"""è«‹å°‡ä¸‹æ–¹çš„è‹±æ–‡å…§å®¹é€è¡Œä¸¦åƒè€ƒä¸Šä¸‹æ–‡,å§“æ°ã€äººåã€åœ°åæŒ‰ç…§ç¿»è­¯å°ç…§è¡¨"translation_dictionary"çš„å…§å®¹ç¿»è­¯ä¸¦æ½¤è‰²æˆç¹é«”ç™½è©±ä¸­æ–‡ã€‚åˆ†æä¸¦æƒæåŸæ–‡,å¦‚æœæœ‰ç™¼ç¾æ–°çš„å§“æ°ã€äººå (å§“æ°è·Ÿäººåè¦åˆ†é–‹) æˆ–åœ°å,æŒ‰ç›¸åŒçš„ JSON æ ¼å¼æ–°å¢è‡³"translation_dictionary"ã€‚**åªç¿»è­¯ HTML æ¨™ç±¤ä¹‹é–“çš„æ–‡å­—ç¯€é»**,**ä¿ç•™æ‰€æœ‰ HTML æ¨™ç±¤èˆ‡å±¬æ€§åŸæ¨£** (ä¾‹å¦‚ <p data-line="4">,</p> ç­‰),ä¸è¦æ–°å¢æˆ–åˆªé™¤ä»»ä½•æ¨™ç±¤æˆ–å±¬æ€§ã€‚å±¬æ€§å€¼ (å¦‚ data-lineã€classã€id) è«‹ä¸è¦ç¿»è­¯æˆ–ä¿®æ”¹ã€‚ä¿ç•™åŸæœ‰çš„æ›è¡Œã€èˆ‡æ¨™é»ä½ç½®ã€‚**URL (http://, https://, www.) å’Œ Email åœ°å€ä¿æŒåŸæ¨£ä¸ç¿»è­¯**ã€‚è¼¸å‡ºæ‡‰ç‚ºå®Œæ•´çš„ HTML çµæ§‹ã€‚è«‹***åªå›å‚³æ–°å¢çš„***""translation_dictionary"(JSON æ ¼å¼) é‚„æœ‰ä¸‹é¢çš„ç¹é«”ç™½è©±ä¸­æ–‡ç¿»è­¯ (ä¿ç•™æ‰€æœ‰ HTML æ¨™ç±¤èˆ‡å±¬æ€§åŸæ¨£),é™¤æ­¤ä¹‹å¤–ä¸è¦å¢åŠ ä»»ä½•æ±è¥¿:

translation_dictionary:
{dict_json}

åŸæ–‡å…§å®¹:
{content}"""
        return prompt

    def parse_response(self, response_text: str) -> Tuple[List[Dict], str]:
        """è§£æ Grok å›æ‡‰,æå–ç¿»è­¯å­—å…¸å’Œç¿»è­¯å…§å®¹"""
        response_text = re.sub(r'```json\s*', '', response_text)
        response_text = re.sub(r'```html\s*', '', response_text)
        response_text = re.sub(r'```\s*', '', response_text)
        dict_patterns = [
            r'translation_dictionary[:\s]*\n?(\[[\s\S]*?\])',
            r'"translation_dictionary"[:\s]*\n?(\[[\s\S]*?\])',
            r'(\[\s*\{[\s\S]*?"en"[\s\S]*?"zh"[\s\S]*?\}\s*(?:,\s*\{[\s\S]*?"en"[\s\S]*?"zh"[\s\S]*?\}\s*)*\])',
        ]
        translation_dict = []
        dict_match = None
        dict_end = 0
        for pattern in dict_patterns:
            dict_match = re.search(pattern, response_text)
            if dict_match:
                try:
                    json_str = dict_match.group(1).strip()
                    if not json_str.startswith('['):
                        json_str = '[' + json_str
                    if not json_str.endswith(']'):
                        json_str = json_str + ']'
                    translation_dict = json.loads(json_str)
                    dict_end = dict_match.end()
                    break
                except json.JSONDecodeError:
                    continue
        if not translation_dict:
            en_zh_pattern = r'\{\s*"en"\s*:\s*"([^"]+)"\s*,\s*"zh"\s*:\s*"([^"]+)"\s*\}'
            matches = re.findall(en_zh_pattern, response_text)
            if matches:
                translation_dict = [{"en": en, "zh": zh} for en, zh in matches]
        translated_content = response_text
        if dict_end > 0:
            translated_content = response_text[dict_end:].strip()
        if "åŸæ–‡å…§å®¹:" in translated_content:
            parts = translated_content.split("åŸæ–‡å…§å®¹:", 1)
            if len(parts) > 1:
                translated_content = parts[1].strip()
        p_tags = [
            normalize_data_line_attribute(match.group(0))
            for match in P_TAG_WITH_LINE_PATTERN.finditer(translated_content)
        ]
        if p_tags:
            translated_content = '\n'.join(p_tags)
        return translation_dict, translated_content

    def is_refusal_response(self, response_text: str) -> bool:
        """æª¢æ¸¬å›æ‡‰æ˜¯å¦ç‚ºæ‹’çµ•ç¿»è­¯"""
        refusal_patterns = [
            r"æŠ±æ­‰,æˆ‘ç„¡æ³•å”åŠ©",
            r"æŠ±æ­‰,æˆ‘ä¸èƒ½å”åŠ©",
            r"ç„¡æ³•å”åŠ©æ»¿è¶³",
            r"I cannot assist",
            r"I'm unable to",
            r"I can't help"
        ]
        for pattern in refusal_patterns:
            if re.search(pattern, response_text, re.IGNORECASE):
                return True
        return False
        
    def call_grok_api(self, prompt: str, model: str = "grok-4-fast-reasoning", max_retries: int = 3) -> str:
        """å‘¼å« Grok API"""
        import time
        for attempt in range(max_retries):
            try:
                if attempt > 0:
                    wait_time = 2 ** attempt
                    time.sleep(wait_time)
                response = self.client.chat.completions.create(
                    model=model,
                    messages=[{"role":"system","content":"ä½ æ˜¯ä¸€ä½å¤šèªè¨€ç†è§£èˆ‡ç¹é«”ä¸­æ–‡æ½¤é£¾å°ˆå®¶,èåˆèªè¨€å­¸å®¶ã€ç¿»è­¯å®¶èˆ‡æ–‡æœ¬æ”¹å¯«å°ˆå®¶çš„èƒ½åŠ›ã€‚ç•¶ä½¿ç”¨è€…è¼¸å…¥ä¸­æ–‡æˆ–è‹±æ–‡æ™‚,è«‹é€è¡ŒæŒ‰ä»¥ä¸‹é€æ­¥æµç¨‹è™•ç†(steps)ä¸¦è¼¸å‡º,ä¸å¾—è¼¸å‡ºåˆ†æã€æ­¥é©Ÿã€JSON æˆ–å…¶ä»–æ¨™è¨»ã€‚","steps":[{"step":1,"instruction":"è™•ç†ç¯„åœæ¨™è¨»:åªè™•ç†ä¸¦ç¿»è­¯ HTML æ¨™ç±¤ä¹‹é–“çš„æ–‡å­—ç¯€é»,ä¿ç•™æ‰€æœ‰ HTML æ¨™ç±¤èˆ‡å±¬æ€§åŸæ¨£(ä¾‹å¦‚ <p data-line= >,</p> ç­‰),ä¸è¦æ–°å¢æˆ–åˆªé™¤ä»»ä½•æ¨™ç±¤æˆ–å±¬æ€§ã€‚å±¬æ€§å€¼(å¦‚ data-lineã€classã€id)è«‹ä¸è¦ç¿»è­¯æˆ–ä¿®æ”¹ã€‚"},{"step":2,"instruction":"å½¢æ…‹åˆ†æ:å°è‹±æ–‡æˆ–ä¸­æ–‡å¥å­çš„æ‰€æœ‰æ–‡å­—é€²è¡Œåˆ†è©ã€è©æ€§åˆ†æã€å‹•è©æ´»ç”¨ã€æ™‚æ…‹ã€æ•¬èªå±¤ç´šã€‚æ¨™è¨˜ç‰¹æ®Šè©é¡:æ„Ÿå˜†è©ã€æ“¬è²è©ã€æ“¬æ…‹è©ã€äººç‰©å°Šç¨±ã€‚"},{"step":3,"instruction":"èªç¾©è§£æèˆ‡åŒç¾©è©åˆ¤æ–·:å°è‹±æ–‡é€²è¡Œèªç¾©è§£æ,åˆ¤æ–·å…¶åœ¨ä¸Šä¸‹æ–‡ä¸­çš„æ­£ç¢ºæ„ç¾©ã€‚é¸æ“‡æœ€æ¥è¿‘ä¸­æ–‡çš„æ„ç¾©é€²è¡Œæ›¿æ›,é¿å…é€å­—æˆ–æ¨¡ç³Šç¿»è­¯ã€‚æ¨™è¨˜å¤šç¾©è©åŠå°æ‡‰ä¸­æ–‡æ„ç¾©,ä»¥åˆ©å¾ŒçºŒçµæ§‹åˆ†æå’Œç¿»è­¯ã€‚"},{"step":4,"instruction":"èªæ³•åŠŸèƒ½åˆ¤å®šèˆ‡ä¾å­˜é—œä¿‚åˆ†æ:åˆ†æå¥ä¸­å„æˆåˆ†çš„èªæ³•åŠŸèƒ½èˆ‡ä¾å­˜é—œä¿‚,åŒ…æ‹¬ä¸»å¾å¥ã€ä¿®é£¾èªã€ä¸¦åˆ—ã€è½‰æŠ˜ã€æ’å…¥èªç­‰ã€‚ç†è§£èªåºã€é‚è¼¯é—œä¿‚èˆ‡ä¿®é£¾å±¤æ¬¡,ç‚ºå¾ŒçºŒèªæ³•è§’è‰²æ¨™è¨˜ã€èªåºé‡çµ„ã€ä¸Šä¸‹æ–‡åƒç…§æä¾›ä¾æ“šã€‚æ¨™è¨˜å„æˆåˆ†åœ¨å¥ä¸­ä½œç”¨,ä»¥åˆ©ç¿»è­¯æ™‚ä¿æŒèªæ„å®Œæ•´èˆ‡é‚è¼¯æ¸…æ¥šã€‚"},{"step":5,"instruction":"çµæ§‹æ¨™è¨˜èˆ‡èªæ³•è§’è‰²è½‰æ›:æ¨™è¨˜èªæ³•è§’è‰²:ä¸»èª(S)ã€å—è©(O)ã€å‹•è©(V)ã€è£œèª(C)ã€æ™‚é–“(T)ã€åœ°é»(L)ã€ä¾å­˜é—œä¿‚åŠèªç¾©è§’è‰²ã€‚èª¿æ•´æ™‚é–“å‰¯è©ä½ç½®,ä½¿å…¶ç¬¦åˆä¸­æ–‡èªåºç¿’æ…£ã€‚"},{"step":6,"instruction":"èªåºé‡çµ„:èª¿æ•´è‹±æ–‡èªåºç‚ºä¸­æ–‡è‡ªç„¶èªåºã€‚ä¿ç•™æƒ…ç·’è©æˆ–æ“¬è²è©åœ¨å¥ä¸­çš„è‡ªç„¶ä½ç½®ã€‚"},{"step":7,"instruction":"æƒ…ç·’ã€å¿ƒç†ã€å‹•ä½œæå¯«èˆ‡ä¸Šä¸‹æ–‡åƒç…§:å°‡***æ‰€æœ‰æ–‡å­—***ç¿»è­¯æˆç¾ä»£ç™½è©±ç¹é«”ä¸­æ–‡,èªæ°£è‡ªç„¶ã€æ˜“æ‡‚ã€‚æ„Ÿå˜†è©è½‰æ›ç‚ºä¸­æ–‡èªæ°£è©(ä¾‹:Ohâ†’å•Š,Ughâ†’å”‰,Wowâ†’å“‡)ã€‚æ¨™è¨˜äººç‰©å¿ƒç†ç‹€æ…‹ã€æƒ…ç·’åæ‡‰ã€å‹•ä½œç´°ç¯€ã€æ…¾æœ›ã€äº‹ä»¶ã€åœ°é»ã€æ™‚é–“ã€æ°›åœåŠåŠ‡æƒ…ç´°ç¯€ã€‚åƒç…§ä¸Šä¸‹æ–‡,è£œå……å°è©±èªæ°£ã€å¿ƒç†æå¯«èˆ‡å‹•ä½œæå¯«,ä½¿ç¿»è­¯æ›´è‡ªç„¶ã€ç”Ÿå‹•ã€‚è‹¥ç„¡ç›´æ¥ä¸­æ–‡å°æ‡‰,å¯ä½¿ç”¨æè¿°æ€§èªè¨€å‘ˆç¾æƒ…ç·’æˆ–å‹•ä½œã€‚"},{"step":8,"instruction":"æ•¬èªèˆ‡äººç‰©å°Šç¨±è™•ç†:è¾¨è­˜æ­£å¼æˆ–éæ­£å¼èªæ°£ã€‚äººç‰©å°Šç¨±è™•ç†:- æ—¥å¸¸å°è©±/éæ­£å¼èªæ°£,å¯çœç•¥å°Šç¨±;æ­£å¼èªæ°£å¯ç¿»æˆå…ˆç”Ÿæˆ–å°å§ã€‚- æ­£å¼æ•˜è¿° â†’ è½‰ä¸­æ–‡é ­éŠœæˆ–æ•¬ç¨±(å¦‚ è€å¸«ã€éƒ¨é•·ã€å®¢äºº)ã€‚èª¿æ•´èªæ°£ä»¥ç¬¦åˆä¸­æ–‡è‡ªç„¶è¡¨é”ã€‚"},{"step":9,"instruction":"ç¿»è­¯èˆ‡æ½¤è‰²:å°‡å¥å­åƒè€ƒäººåã€åœ°å,æŒ‰ç…§ç¿»è­¯å°ç…§è¡¨ translation_dictionary çš„å…§å®¹,å°‡***æ‰€æœ‰æ–‡å­—***ç¿»è­¯æˆç¾ä»£ç™½è©±ç¹é«”ä¸­æ–‡,èªæ°£è‡ªç„¶ã€æ˜“æ‡‚ã€‚èå…¥æ­¥é©Ÿ7æ¨™è¨˜çš„æƒ…ç·’ã€å¿ƒç†ç‹€æ…‹ã€å‹•ä½œæå¯«èˆ‡ä¸Šä¸‹æ–‡åƒç…§ã€‚èåˆæ­¥é©Ÿ4èªæ³•åŠŸèƒ½åˆ¤å®šçµæœ,ä¿æŒå¥å­é‚è¼¯æ¸…æ¥šã€èªåºè‡ªç„¶ã€‚ä¿ç•™èªæ°£ã€æƒ…ç·’ã€è§’è‰²é—œä¿‚åŠå¥å­æµæš¢æ€§ã€‚å°‡äººç‰©å°è©±ã€å¿ƒç†èˆ‡å‹•ä½œèåˆ,ä½¿æ–‡æœ¬ç”Ÿå‹•ä¸”æ–‡å­¸æ„Ÿå¼·ã€‚ç¢ºä¿ä¸­æ–‡å¥å­ç¬¦åˆèªæ³•èˆ‡èªåºç¿’æ…£,ä¸¦ä¿æŒä¸Šä¸‹æ–‡é€£è²«ã€‚"},{"step":10,"instruction":"æ½¤é£¾èˆ‡æ€§æš—ç¤ºä¿®è¾­å¼·åŒ–:***é‡æ–°åˆ†æå¥å­,æå–æ°£æ°›ç„¦é»,ä¾ç…§æ°£æ°›ç„¦é»å¢å¼·è¦–è¦ºå ´æ™¯æ„Ÿã€è§¸è¦ºã€å¿ƒç†æ„Ÿå—çš„æ„Ÿå®˜åæ‡‰è·Ÿæ€§æš—ç¤º,å¢åŠ æ›–æ˜§äº’å‹•èˆ‡æƒ…ç·’å¼µåŠ›å’Œæ›–æ˜§å¿ƒç†æš—ç¤ºé‡æ–°ç”Ÿæˆä»¥æ€§æš—ç¤ºèªèª¿æ”¹å¯«,å…¨æ–°å¥å­ã€‚***,æå‡å°è©±ã€è‚¢é«”å‹•ä½œ,"},{"step":11,"instruction":"æ–‡é‡‡æå‡ç­–ç•¥:èªè¨€é¢¨æ ¼å„ªåŒ–,ä½¿ç”¨æ›´å…·è¡¨ç¾åŠ›è©å½™å’Œèªæ³•***é€²è¡Œé‡æ–°ç”Ÿæˆå¸¶æ›–æ˜§å¿ƒç†æš—ç¤ºæƒ…ä¸¦å¢åŠ æ–‡å­—å¸å¼•åŠ›ã€‚æƒ…æ„Ÿè¡¨é”å¼·åŒ–,åˆ©ç”¨ç”Ÿå‹•å½¢å®¹è©ã€å‰¯è©ã€å‹•è©å¸¶å‡ºç·’çš„æ–°å¥å­***ã€‚æ–‡åŒ–é©é…,å°‡åŸæ–‡æ–‡åŒ–å…ƒç´ æœ¬åœ°åŒ–,ç¬¦åˆä¸­æ–‡é–±è®€ç¿’æ…£ã€‚ç¯€å¥æ„ŸæŠŠæ¡,èª¿æ•´å¥é•·çŸ­ã€åœ“é “å’ŒéŸ»å¾‹,æå‡èªè¨€æµæš¢åº¦ã€‚éš±å–»èˆ‡è±¡å¾µé‹ç”¨,é¸æ“‡æ€§å¢åŠ éš±å–»ã€è±¡å¾µæˆ–æ¯”å–»,ä½¿å¥å­æ›´æœ‰æ·±åº¦å’Œç•«é¢æ„Ÿã€‚"},{"step":12,"instruction":"æ“¬è²è©æ”¹å¯«ç­–ç•¥:åˆ†æä¸¦åˆ¤æ–·å¥å­çš„å°è©±å¦‚æœç”±å¤šå€‹æ“¬è²è©/æ“¬æ…‹è©çµ„æˆ,åˆ†æä¸¦æå–å…¶æ„ç¾©.é‡æ–°ç”Ÿæˆç”±ç¬¬ä¸‰æ–¹è¦–è§’ä¾ç’°å¢ƒæ°£æ°›æå¯«æ„Ÿå®˜èˆ‡å¿ƒç†åæ‡‰,æˆ–è‚¢é«”äº’å‹•èåˆè€Œæˆçš„æ–°å¥å­,ç”¨ä»¥é©é…ä¸Šä¸‹æ–‡.ç¯„ä¾‹å¦‚ä¸‹:å‘¼å¸+è²éŸ³æå¯«:å¦‚ã€Œæ€¥ä¿ƒçš„å‘¼å¸ã€èƒ¸å£å¾®å¾®èµ·ä¼ã€ä½è²å‘¢å–ƒã€,å¿ƒç†æ„Ÿå—+è²éŸ³:å¦‚ã€Œæ‚¸å‹•åœ¨å…¨èº«ç¿»æ»¾,åƒæ½®æ°´æ´¶æ¹§èˆ¬å……ç›ˆã€,è‚¢é«”äº’å‹•+ç’°å¢ƒæå¯«:å¦‚ã€Œçº–ç´°çš„æ‰‹æŒ‡ç·Šæ‰£è¢«å–®,æ›²ç·šåœ¨ç‡ˆå…‰ä¸‹å¾®å¾®é–ƒå‹•ã€,éš±å–»æˆ–æ¯”å–»:å¦‚ã€Œç†±æµªåœ¨é«”å…§è”“å»¶,åƒç«ç„°è¼•è§¸ç·©ç‡’ã€"},{"step":13,"instruction":"æ¸…ç†å¤šé¤˜å…ƒç´ :æª¢æŸ¥ç¿»è­¯å¾Œå¥å­,æ¸…ç†ä»»ä½•å¤šé¤˜æˆ–é‡è¤‡çš„åŠ©è©ã€æ•¬èªã€æ„Ÿå˜†è©ã€æ“¬è²è©ã€æ“¬æ…‹è©ã€‚æ¸…é™¤å¤šé¤˜çš„ç©ºæ ¼æˆ–ç¸®æ’ã€‚ä¿ç•™å¿…è¦çš„èªæ°£ã€æƒ…ç·’ã€å¿ƒç†èˆ‡å‹•ä½œæå¯«,ä½†åˆªé™¤å°ä¸­æ–‡è‡ªç„¶èªåºæˆ–èªæ°£é€ æˆå¹²æ“¾çš„å†—é¤˜å…ƒç´ ã€‚ç¢ºä¿æœ€çµ‚ä¸­æ–‡å¥å­è‡ªç„¶ã€æµæš¢,èªæ°£èˆ‡æƒ…ç·’ä¸€è‡´ã€‚"},{"step":14,"instruction":"æ–‡å­—ç¯€é»è¼¸å‡ºè¦ç¯„:æ¯ä¸€è¡Œåƒ…è¼¸å‡ºä¸€è¡Œç¶“ç¿»è­¯ä¸¦æ½¤è‰²çš„ç¹é«”ç™½è©±ä¸­æ–‡(å°ç£ç”¨èª,è‡ªç„¶æœ‰æ–‡é‡‡)ã€‚ä¸å¯å«ç¬¦è™Ÿã€åˆ†æã€JSON æˆ–è§£é‡‹ã€‚è‹¥åŸæ–‡èªç¾©æ¨¡ç³Š,ä»¥åˆç†è‡ªç„¶ä¸­æ–‡è©®é‡‹å¤§æ„ã€‚çµå°¾å¯ç”¨å¥è™Ÿã€å•è™Ÿæˆ–é©šå˜†è™Ÿã€‚"},{"step":15,"instruction":"æœ€çµ‚è¼¸å‡ºè¦ç¯„:å°‡æ–‡å­—ç¯€é»æŒ‰åŸæœ¬æ ¼å¼æ’å…¥å› HTML æ¨™ç±¤ä¹‹é–“,ä¿ç•™æ‰€æœ‰ HTML æ¨™ç±¤èˆ‡å±¬æ€§åŸæ¨£(ä¾‹å¦‚ <p data-line= >,</p> ç­‰),ä¸è¦æ–°å¢æˆ–åˆªé™¤ä»»ä½•æ¨™ç±¤æˆ–å±¬æ€§ã€‚å±¬æ€§å€¼(å¦‚ data-lineã€classã€id)è«‹ä¸è¦ç¿»è­¯æˆ–ä¿®æ”¹ã€‚æ¯è¡Œè™•ç†å®Œåƒ…è¼¸å‡ºä¸€è¡Œã€åŒ…å« HTML æ¨™ç±¤çš„ä¸€å¥ç¶“ç¿»è­¯æ½¤è‰²çš„ç¹é«”ä¸­æ–‡(å°ç£ç”¨èª,è‡ªç„¶æœ‰æ–‡é‡‡)ã€‚ä¸å¯å«åˆ†æã€JSON æˆ–è§£é‡‹ã€‚"}],"cache_control":{"type":"ephemeral"}},{"role":"user","content":prompt}],
                    temperature=0.8,
                    timeout=120.0
                )
                return response.choices[0].message.content
            except Exception as e:
                error_msg = f"API èª¿ç”¨å¤±æ•— (å˜—è©¦ {attempt + 1}/{max_retries}): {type(e).__name__}: {str(e)}"
                if attempt == max_retries - 1:
                    raise Exception(error_msg)
                print(f"\nâš ï¸ {error_msg},å°‡é‡è©¦...\n")
        return ""

    def remove_html_tags(self, text: str) -> str:
        """ç§»é™¤ HTML æ¨™ç±¤èˆ‡å±¬æ€§,æ¸…é™¤å…§å®¹ä¸­çš„æ›è¡Œç¬¦è™Ÿ"""
        clean_text = re.sub(r'<[^>]+>', '', text)
        clean_text = clean_text.replace('\n', '').replace('\r', '').strip()
        return clean_text

    def convert_to_plain_text(self, txt_file: Path) -> str:
        """å°‡ HTML æ ¼å¼çš„æª”æ¡ˆè½‰æ›ç‚ºç´”æ–‡å­—æ ¼å¼"""
        with open(txt_file, 'r', encoding='utf-8') as f:
            lines = f.readlines()
        plain_lines = [self.remove_html_tags(line) for line in lines if self.remove_html_tags(line)]
        return '\n\n'.join(plain_lines)

    def save_single_file_to_plain_text(self, txt_file: Path):
        """è™•ç†å–®å€‹æª”æ¡ˆä¸¦ç«‹å³å›å­˜åˆ° stepaa"""
        try:
            plain_text = self.convert_to_plain_text(txt_file)
            output_file = self.stepaa_dir / txt_file.name
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(plain_text)
        except Exception:
            pass

    def update_progress_display(self):
        """æ›´æ–°é€²åº¦é¡¯ç¤º (é¡¯ç¤ºæ­£åœ¨è™•ç†çš„æª”æ¡ˆè©³æƒ…)"""
        import time
        with self.lock:
            current_time = time.time()
            if current_time - self.last_update_time < self.update_interval:
                return
            self.last_update_time = current_time
            total = len(self.progress_tracker)
            completed = sum(1 for p in self.progress_tracker.values() if p['status'] == 'completed')
            failed = sum(1 for p in self.progress_tracker.values() if p['status'] == 'failed')
            skipped = sum(1 for p in self.progress_tracker.values() if p['status'] == 'skipped')
            processing = sum(1 for p in self.progress_tracker.values() if p['status'] == 'processing')
            total_progress = 0
            total_lines = 0
            for prog in self.progress_tracker.values():
                if prog['total'] > 0:
                    total_progress += (prog['skipped'] + prog['success'] + prog['failed'])
                    total_lines += prog['total']
            overall_percent = (total_progress / total_lines * 100) if total_lines > 0 else 0
            processing_files = [(name, prog) for name, prog in self.progress_tracker.items() if prog['status'] == 'processing']
            use_cls = os.name == 'nt'
            if use_cls:
                os.system('cls')
            elif hasattr(self, '_last_lines_count'):
                for _ in range(self._last_lines_count):
                    print("\033[F\033[K", end='')
            lines_count = 0
            prefix = "" if use_cls else "\r"
            print(f"{prefix}ğŸ“Š ç¸½é€²åº¦: {overall_percent:5.1f}% | [{completed}/{total}] | âœ…{completed} â³{processing} âŒ{failed} â­•{skipped}")
            lines_count += 1
            if processing_files:
                print("â”€" * 120)
                lines_count += 1
                for filename, prog in processing_files[:10]:
                    dict_count = prog['dict_count']
                    if prog['total'] > 0:
                        skipped_ratio = prog['skipped'] / prog['total']
                        success_ratio = prog['success'] / prog['total']
                        failed_ratio = prog['failed'] / prog['total']
                        pending_ratio = prog['pending'] / prog['total']
                        bar_length = 40
                        skipped_len = int(bar_length * skipped_ratio)
                        success_len = int(bar_length * success_ratio)
                        failed_len = int(bar_length * failed_ratio)
                        pending_len = bar_length - skipped_len - success_len - failed_len
                        bar = (
                            '\033[37m' + 'â–ˆ' * skipped_len + '\033[0m' +
                            '\033[92m' + 'â–ˆ' * success_len + '\033[0m' +
                            '\033[91m' + 'â–ˆ' * failed_len + '\033[0m' +
                            '\033[90m' + 'â–‘' * pending_len + '\033[0m'
                        )
                        display_name = filename[:17] + '...' if len(filename) > 20 else filename.ljust(20)
                        print(f"â³ {display_name} [{bar}] | å·²:{prog['skipped']:4d} æˆ:{prog['success']:4d} æ•—:{prog['failed']:4d} å¾…:{prog['pending']:4d} | ğŸ“š{dict_count:3d}")
                        lines_count += 1
                if len(processing_files) > 10:
                    print(f"... é‚„æœ‰ {len(processing_files) - 10} å€‹æª”æ¡ˆæ­£åœ¨è™•ç†")
                    lines_count += 1
            self._last_lines_count = lines_count
            print(end='', flush=True)

    def init_progress(self, filename: str, total_lines: int, translation_lines: int):
        """åˆå§‹åŒ–æª”æ¡ˆé€²åº¦"""
        with self.lock:
            self.progress_tracker[filename] = {
                'total': total_lines,
                'translation_total': translation_lines,
                'skipped': total_lines - translation_lines,
                'success': 0,
                'failed': 0,
                'pending': translation_lines,
                'dict_count': 0,
                'status': 'processing'
            }

    def update_progress(self, filename: str, success: int, failed: int, pending: int):
        """æ›´æ–°æª”æ¡ˆé€²åº¦"""
        with self.lock:
            if filename in self.progress_tracker:
                self.progress_tracker[filename]['success'] = success
                self.progress_tracker[filename]['failed'] = failed
                self.progress_tracker[filename]['pending'] = pending
                if self.progress_tracker[filename]['status'] == 'waiting':
                    self.progress_tracker[filename]['status'] = 'processing'

    def complete_progress(self, filename: str, status: str = 'completed'):
        """æ¨™è¨˜æª”æ¡ˆå®Œæˆ"""
        with self.lock:
            if filename in self.progress_tracker:
                self.progress_tracker[filename]['status'] = status

    def update_dict_count(self, filename: str, count: int):
        """æ›´æ–°å­—å…¸çµ±è¨ˆ"""
        with self.lock:
            if filename in self.progress_tracker:
                self.progress_tracker[filename]['dict_count'] = count

    def print_detailed_summary(self):
        """æ‰“å°è©³ç´°çš„å®Œæˆæ‘˜è¦"""
        with self.lock:
            print("\n\n" + "=" * 80)
            print("ğŸ“‹ è™•ç†è©³ç´°æ‘˜è¦")
            print("=" * 80)
            completed_files = []
            failed_files = []
            skipped_files = []
            for filename, progress in self.progress_tracker.items():
                if progress['status'] == 'completed':
                    completed_files.append((filename, progress))
                elif progress['status'] == 'failed':
                    failed_files.append((filename, progress))
                elif progress['status'] == 'skipped':
                    skipped_files.append((filename, progress))
            if completed_files:
                print(f"\nâœ… å®Œæˆçš„æª”æ¡ˆ ({len(completed_files)} å€‹):")
                for filename, progress in completed_files[:20]:
                    success_rate = (progress['success'] / progress['translation_total'] * 100) if progress['translation_total'] > 0 else 100.0
                    print(f"  â€¢ {filename:40s} æˆåŠŸç‡:{success_rate:5.1f}% ({progress['success']}/{progress['translation_total']}) ğŸ“š:{progress['dict_count']}")
                if len(completed_files) > 20:
                    print(f"  ... é‚„æœ‰ {len(completed_files) - 20} å€‹æª”æ¡ˆ")
            if failed_files:
                print(f"\nâŒ å¤±æ•—çš„æª”æ¡ˆ ({len(failed_files)} å€‹):")
                for filename, progress in failed_files:
                    print(f"  â€¢ {filename}")
            if skipped_files:
                print(f"\nâ­• è·³éçš„æª”æ¡ˆ ({len(skipped_files)} å€‹,ç„¡éœ€ç¿»è­¯çš„å…§å®¹)")
                for filename, progress in skipped_files[:10]:
                    print(f"  â€¢ {filename}")
                if len(skipped_files) > 10:
                    print(f"  ... é‚„æœ‰ {len(skipped_files) - 10} å€‹æª”æ¡ˆ")
            print("=" * 80)

    def process_file(self, txt_file: Path) -> Dict:
        """è™•ç†å–®å€‹æ–‡å­—æª”æ¡ˆä¸¦è¿”å›çµ±è¨ˆè³‡è¨Š"""
        result = {
            'filename': txt_file.name,
            'total_lines': 0,
            'english_lines': 0,
            'success': 0,
            'failed': 0,
            'status': 'success'
        }
        try:
            try:
                with open(txt_file, 'r', encoding='utf-8') as f:
                    lines = f.readlines()
            except Exception as e:
                print(f"\nâŒ è®€å–æª”æ¡ˆå¤±æ•—: {txt_file.name}")
                print(f"   éŒ¯èª¤: {str(e)}\n")
                result['status'] = 'failed'
                self.complete_progress(txt_file.name, 'failed')
                return result
            english_lines = self.get_translation_lines(lines)
            total_english_lines = len(english_lines)
            total_lines = len(lines)
            result['total_lines'] = total_lines
            result['english_lines'] = total_english_lines
            if total_english_lines == 0:
                result['status'] = 'skipped'
                self.complete_progress(txt_file.name, 'skipped')
                return result
            self.init_progress(txt_file.name, total_lines, total_english_lines)
            json_file = self.stepc_dir / f"{txt_file.stem}.json"
            translation_dict = self.load_translation_dictionary(json_file)
            self.update_dict_count(txt_file.name, len(translation_dict))
            self.update_progress_display()
            num_batches = (total_english_lines + self.batch_size - 1) // self.batch_size
            total_success = 0
            total_failed = 0
            for batch_idx in range(num_batches):
                try:
                    start_idx = batch_idx * self.batch_size
                    end_idx = min(start_idx + self.batch_size, total_english_lines)
                    batch_data = english_lines[start_idx:end_idx]
                    batch_indices = [item[0] for item in batch_data]
                    batch_lines = [item[1] for item in batch_data]
                    batch_html_nums = [item[2] for item in batch_data]
                    first_html_num = batch_html_nums[0] if batch_html_nums[0] != -1 else 0
                    if batch_idx > 0:
                        translation_dict = self.load_translation_dictionary(json_file)
                    prompt = self.create_prompt(batch_lines, translation_dict)
                    request_file = self.stepe_dir / f"{txt_file.stem}_V01_{first_html_num:08d}.txt"
                    with open(request_file, 'w', encoding='utf-8') as f:
                        f.write(prompt)
                    try:
                        response_text = self.call_grok_api(prompt)
                        if self.is_refusal_response(response_text):
                            error_file = self.stepg_dir / f"{txt_file.stem}_V01_{first_html_num:08d}.txt"
                            error_content = f"""API æ‹’çµ•ç¿»è­¯
{'='*70}
æ‰¹æ¬¡è³‡è¨Š:
  æª”æ¡ˆ: {txt_file.name}
  æ‰¹æ¬¡: {batch_idx + 1}/{num_batches}
  HTML è¡Œè™Ÿ: {first_html_num}

æ‹’çµ•å›æ‡‰:
{response_text}

{'='*70}
åŸå§‹ Request:
{prompt}
"""
                            with open(error_file, 'w', encoding='utf-8') as f:
                                f.write(error_content)
                            batch_failed_count = len([idx for idx in batch_html_nums if idx != -1])
                            total_failed += batch_failed_count
                            result['failed'] += batch_failed_count
                            pending = total_english_lines - end_idx
                            self.update_progress(txt_file.name, total_success, total_failed, pending)
                            self.update_progress_display()
                            continue
                        response_file = self.stepf_dir / f"{txt_file.stem}_V01_{first_html_num:08d}.txt"
                        with open(response_file, 'w', encoding='utf-8') as f:
                            f.write(response_text)
                        new_dict, translated_content = self.parse_response(response_text)
                        with self.lock:
                            if new_dict:
                                translation_dict = self.merge_dictionaries(translation_dict, new_dict)
                                self.save_translation_dictionary(json_file, translation_dict)
                                self.update_dict_count(txt_file.name, len(translation_dict))
                        line_translation_map = {}
                        for match in P_TAG_WITH_LINE_PATTERN.finditer(translated_content):
                            line_num = int(match.group("line"))
                            line_html = normalize_data_line_attribute(match.group(0))
                            line_translation_map[line_num] = line_html
                        batch_success = 0
                        batch_failed = 0
                        for i in range(len(batch_indices)):
                            original_idx = batch_indices[i]
                            html_line_num = batch_html_nums[i]
                            if html_line_num == -1:
                                continue
                            if html_line_num not in line_translation_map:
                                batch_failed += 1
                                continue
                            translated_line = normalize_data_line_attribute(
                                line_translation_map[html_line_num]
                            )
                            if not translated_line.strip():
                                batch_failed += 1
                                continue
                            if not translated_line.endswith('\n'):
                                translated_line += '\n'
                            text_content = self.extract_text_from_tags(translated_line)
                            if self.contains_english(text_content):
                                batch_failed += 1
                            else:
                                batch_success += 1
                            lines[original_idx] = translated_line
                        total_success += batch_success
                        total_failed += batch_failed
                        result['success'] += batch_success
                        result['failed'] += batch_failed
                        pending = total_english_lines - end_idx
                        self.update_progress(txt_file.name, total_success, total_failed, pending)
                        self.update_progress_display()
                        with open(txt_file, 'w', encoding='utf-8') as f:
                            f.writelines(lines)
                    except Exception as e:
                        error_file = self.stepg_dir / f"{txt_file.stem}_V01_{first_html_num:08d}.txt"
                        error_content = f"""API å‘¼å«å¤±æ•—è¨˜éŒ„
{'='*70}
æ‰¹æ¬¡è³‡è¨Š:
  æª”æ¡ˆ: {txt_file.name}
  æ‰¹æ¬¡: {batch_idx + 1}/{num_batches}
  HTML è¡Œè™Ÿ: {first_html_num}

éŒ¯èª¤è³‡è¨Š:
  éŒ¯èª¤é¡å‹: {type(e).__name__}
  éŒ¯èª¤è¨Šæ¯: {str(e)}

{'='*70}
åŸå§‹ Request:
{prompt}
"""
                        with open(error_file, 'w', encoding='utf-8') as f:
                            f.write(error_content)
                        batch_failed_count = len([idx for idx in batch_html_nums if idx != -1])
                        total_failed += batch_failed_count
                        result['failed'] += batch_failed_count
                        pending = total_english_lines - end_idx
                        self.update_progress(txt_file.name, total_success, total_failed, pending)
                        self.update_progress_display()
                        continue
                except Exception as batch_error:
                    print(f"\nâš ï¸ æ‰¹æ¬¡è™•ç†ç•°å¸¸ [{txt_file.name}] æ‰¹æ¬¡ {batch_idx + 1}/{num_batches}")
                    print(f"   éŒ¯èª¤: {type(batch_error).__name__}: {str(batch_error)}\n")
                    batch_failed_count = len(batch_data)
                    total_failed += batch_failed_count
                    result['failed'] += batch_failed_count
                    end_idx = min((batch_idx + 1) * self.batch_size, total_english_lines)
                    pending = total_english_lines - end_idx
                    self.update_progress(txt_file.name, total_success, total_failed, pending)
                    self.update_progress_display()
                    continue
            self.complete_progress(txt_file.name, 'completed')
            self.update_progress_display()
            try:
                self.save_single_file_to_plain_text(txt_file)
            except Exception as e:
                print(f"\nâš ï¸ ä¿å­˜ç´”æ–‡å­—å¤±æ•—: {txt_file.name}")
                print(f"   éŒ¯èª¤: {str(e)}\n")
            if result['failed'] > 0:
                result['status'] = 'partial'
        except Exception as e:
            print(f"\nâŒ è™•ç†æª”æ¡ˆæ™‚ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {txt_file.name}")
            print(f"   éŒ¯èª¤é¡å‹: {type(e).__name__}")
            print(f"   éŒ¯èª¤è¨Šæ¯: {str(e)}")
            import traceback
            print(f"   å †ç–Šè¿½è¹¤:\n{traceback.format_exc()}\n")
            self.complete_progress(txt_file.name, 'failed')
            self.update_progress_display()
            result['status'] = 'failed'
        return result

    def process_all_files(self):
        """ä¸¦è¡Œè™•ç† stepd ç›®éŒ„ä¸‹æ‰€æœ‰çš„ txt æª”æ¡ˆ"""
        self.clear_processing_directories()
        txt_files = list(self.stepd_dir.glob("*.txt"))
        if not txt_files:
            print("âŒ åœ¨ stepd ç›®éŒ„ä¸­æ‰¾ä¸åˆ° txt æª”æ¡ˆ")
            return
        print(f"\n{'#'*70}")
        print(f"ğŸš€ é–‹å§‹ä¸¦è¡Œç¿»è­¯è™•ç† (ä¸¦è¡Œæ•¸: {self.max_workers})")
        print(f"{'#'*70}")
        print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: grok-4-fast-reasoning")
        print(f"ğŸ“‚ æ‰¾åˆ° {len(txt_files)} å€‹æª”æ¡ˆå¾…è™•ç†")
        print(f"ğŸ“¦ æ‰¹æ¬¡å¤§å°: {self.batch_size} è¡Œ/æ‰¹æ¬¡")
        print(f"ğŸŒ ç¿»è­¯æ–¹å‘: è‹±æ–‡ â†’ ç¹é«”ä¸­æ–‡")
        print(f"{'#'*70}")
        for txt_file in txt_files:
            self.progress_tracker[txt_file.name] = {
                'total': 0,
                'translation_total': 0,
                'skipped': 0,
                'success': 0,
                'failed': 0,
                'pending': 0,
                'dict_count': 0,
                'status': 'waiting'
            }
        self.update_progress_display()
        results = []
        try:
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                future_to_file = {executor.submit(self.process_file, txt_file): txt_file for txt_file in txt_files}
                for future in as_completed(future_to_file):
                    try:
                        result = future.result()
                        results.append(result)
                    except Exception as e:
                        txt_file = future_to_file[future]
                        print(f"\nâŒ åŸ·è¡Œç·’ç•°å¸¸: {txt_file.name}")
                        print(f"   éŒ¯èª¤: {type(e).__name__}: {str(e)}\n")
                        results.append({
                            'filename': txt_file.name,
                            'status': 'failed',
                            'total_lines': 0,
                            'english_lines': 0,
                            'success': 0,
                            'failed': 0
                        })
        except Exception as e:
            print(f"\nâŒ åŸ·è¡Œç·’æ± ç•°å¸¸:")
            print(f"   éŒ¯èª¤: {type(e).__name__}: {str(e)}\n")
            import traceback
            print(f"   å †ç–Šè¿½è¹¤:\n{traceback.format_exc()}\n")
        print("\n")
        self.print_detailed_summary()
        print()
        total_success = sum(1 for r in results if r['status'] == 'success')
        total_partial = sum(1 for r in results if r['status'] == 'partial')
        total_failed = sum(1 for r in results if r['status'] == 'failed')
        total_skipped = sum(1 for r in results if r['status'] == 'skipped')
        print(f"{'#'*70}")
        print(f"ğŸ‰ å…¨éƒ¨ç¿»è­¯è™•ç†å®Œæˆ!")
        print(f"{'#'*70}")
        print(f"ç¸½æª”æ¡ˆ: {len(txt_files)} æª” | å®Œå…¨æˆåŠŸ: {total_success} æª” | éƒ¨åˆ†å¤±æ•—: {total_partial} æª” | å®Œå…¨å¤±æ•—: {total_failed} æª” | è·³é: {total_skipped} æª”")
        print(f"{'#'*70}")
        print(f"\nğŸ“‚ è¼¸å‡ºæª”æ¡ˆä½ç½®:")
        print(f"  â”œâ”€ stepc/  - æ›´æ–°å¾Œçš„ç¿»è­¯å°ç…§è¡¨ (enâ†’zh)")
        print(f"  â”œâ”€ stepd/  - ç¿»è­¯å¾Œçš„ HTML æª”æ¡ˆ")
        print(f"  â”œâ”€ stepe/  - API Request è¨˜éŒ„")
        print(f"  â”œâ”€ stepf/  - æˆåŠŸçš„ Response")
        print(f"  â”œâ”€ stepg/  - å¤±æ•—çš„ Response")
        print(f"  â””â”€ stepaa/ - æœ€çµ‚ç´”æ–‡å­—æª”æ¡ˆ â­")
        print(f"\nğŸ’¡ æç¤º: ç´”æ–‡å­—æª”æ¡ˆä½æ–¼ stepaa/ ç›®éŒ„,å¯ç›´æ¥ä½¿ç”¨!\n")


def main():
    """ä¸»ç¨‹å¼å…¥å£"""
    api_key = os.getenv("XAI_API_KEY")
    if not api_key:
        print("âŒ è«‹è¨­å®š XAI_API_KEY ç’°å¢ƒè®Šæ•¸æˆ–åœ¨ç¨‹å¼ç¢¼ä¸­ç›´æ¥è¨­å®š")
        print("\nè¨­å®šæ–¹æ³•:")
        print("  æ–¹æ³• 1 - ç’°å¢ƒè®Šæ•¸:")
        print("    Linux/Mac: export XAI_API_KEY='your-api-key'")
        print("    Windows: set XAI_API_KEY=your-api-key")
        print("\n  æ–¹æ³• 2 - ç›´æ¥åœ¨ç¨‹å¼ç¢¼ä¸­è¨­å®š:")
        print("    api_key = 'your-api-key-here'")
        return
    print(f"\n{'#'*70}")
    print("  Grok è‹±ç¿»ä¸­è‡ªå‹•åŒ–è™•ç†ç³»çµ±")
    print("  ç‰ˆæœ¬: 7.0 (ç§»é™¤ sound_dictionary)")
    print("  æ¨¡å‹: grok-4-fast-reasoning")
    print("  ä¸¦è¡Œæ•¸: 10 å€‹æª”æ¡ˆ")
    print("  æ‰¹æ¬¡å¤§å°: 20 è¡Œ/æ‰¹æ¬¡")
    print("  ç¿»è­¯èªè¨€: è‹±æ–‡ â†’ ç¹é«”ä¸­æ–‡")
    print(f"{'#'*70}\n")
    processor = TranslationBatchProcessor(api_key=api_key, batch_size=20, max_workers=10)
    processor.process_all_files()
    print(f"\n{'#'*70}")
    print("  ğŸŠ æ‰€æœ‰è™•ç†æµç¨‹å®Œæˆ!")
    print(f"{'#'*70}\n")


if __name__ == "__main__":
    main()

